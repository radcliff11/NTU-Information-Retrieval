{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~For N labels =  750 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.79491584852735 94.96666666666667 97.32094375106858\n",
      "Naive Bayes Accuracy Score ->  94.96666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 95.06666666666666 97.47095010252905\n",
      "SVM Accuracy Score ->  95.06666666666666\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 95.06666666666666 97.47095010252905\n",
      "LR Accuracy Score ->  95.06666666666666\n",
      "Time spent on tokenizing for bag of words:  208.67382144927979 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5302584171295166 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.04491019248962402 / 10.527640581130981 / 0.14460492134094238 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#Set Random seed\n",
    "np.random.seed(500)\n",
    "\n",
    "\n",
    "\n",
    "#Read unclean just processed tweet\n",
    "df = tweets = pd.read_excel(\"./healthcare30000.xlsx\")\n",
    "df = df[['TweetText','Polarity']]\n",
    "stop_words = set(['a','about','above','after','again','against','all','am','an','and','any','are','aren\\'t','as','at','be','because','been','before','being','below','between','both','but','by','can\\'t','cannot','could','couldn\\'t','did','didn\\'t','do','does','doesn\\'t','doing','don\\'t','down','during','each','few','for','from','further','had','hadn\\'t','has','hasn\\'t','have','haven\\'t','having','he','he\\'d','he\\'ll','he\\'s','her','here','here\\'s','hers','herself','him','himself','his','how','how\\'s','i','i\\'d','i\\'ll','i\\'m','i\\'ve','if','in','into','is','isn\\'t','it','it\\'s','its','itself','let\\'s','me','more','most','mustn\\'t','my','myself','no','nor','not','of','off','on','once','only','or','other','ought','our','ours', 'ourselves','out','over','own','same','shan\\'t','she','she\\'d','she\\'ll','she\\'s','should','shouldn\\'t','so','some','such','than','that','that\\'s','the','their','theirs','them','themselves','then','there','there\\'s','these','they','they\\'d','they\\'ll','they\\'re','they\\'ve','this','those','through','to','too','under','until','up','very','was','wasn\\'t','we','we\\'d','we\\'ll','we\\'re','we\\'ve','were','weren\\'t','what','what\\'s','when','when\\'s','where','where\\'s','which','while','who','who\\'s','whom','why','why\\'s','with','won\\'t','would','wouldn\\'t','you','you\\'d','you\\'ll','you\\'re','you\\'ve','your','yours','yourself','yourselves'])\n",
    "\n",
    "def processRow(row):\n",
    "    tweet = row.lower()    #Lower case\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)    #delete any url\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) #delete any @Username\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)#Remove additional white spaces\n",
    "    tweet = re.sub('[\\n]+', ' ', tweet) #Remove not alphanumeric symbols white spaces\n",
    "    tweet = re.sub(r'[^\\w]', ' ', tweet) #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #Remove Digits\n",
    "    tweet = re.sub(\" \\d+\", '', tweet)\n",
    "    tweet = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", tweet)\n",
    "    tweet = tweet.replace(':)','')    #Remove :( or :)\n",
    "    tweet = tweet.replace(':(','')\n",
    "    tweet = tweet.strip('\\'\"')    #trim\n",
    "    tweet = [word for word in tweet.split() if word not in stop_words and len(word) > 1]#Removes stopwords and single letter words\n",
    "    return ''.join(str(e) + \" \" for e in tweet)\n",
    "testing = np.array(list(df['TweetText'][:30000]))  \n",
    "for x in range(0,testing.shape[0]):\n",
    "    testing[x] = processRow(testing[x])\n",
    "\n",
    "#print(df.TweetText = testing)\n",
    "\n",
    "### Write Pre-processed with stopwords Data into CSV file (uncomment to write)\n",
    "###df.to_csv(\"clean_stopwordsremoved_healthcaretweet30000.csv\", index = False)\n",
    "def calPerformanceofModels(path,label,max_features):\n",
    "    # Add the Data using pandas\n",
    "    start = time.time()\n",
    "    Corpus = pd.read_csv(path,encoding='latin-1')\n",
    "    Corpus['Polarity'] = Corpus['Polarity'].apply(str) #converts the float string into string/obj for processing\n",
    "    Corpus.dropna()\n",
    "    #print(Corpus.shape)\n",
    "    # Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    Corpus['TweetText']= [word_tokenize(str(entry)) for entry in Corpus['TweetText']]\n",
    "    # Step - 1b: Perfom Word Stemming/Lemmenting.\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(Corpus['TweetText']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "    #print(Corpus['text_final'].head())\n",
    "    end = time.time()\n",
    "    tok_time = end-start\n",
    "    \n",
    "    # Split the model into Train and Test Data set\n",
    "    start  = time.time()\n",
    "    Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['Polarity'],test_size=0.2)\n",
    "    # Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "    # Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comaprison to the corpus\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=max_features)\n",
    "    Tfidf_vect.fit(Corpus['text_final'])\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "    # Now we can run different algorithms to classify our data check for accuracy\n",
    "    end  = time.time()\n",
    "    vect_time = end-start\n",
    "    # Classifier - Algorithm - Naive Bayes\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time()\n",
    "    \n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf) # predict the labels on validation dataset\n",
    "    \n",
    "    end  = time.time()\n",
    "    nb_time = end -start\n",
    "    #NAIVE BAYES END\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time()\n",
    "    \n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)    # predict the labels on validation dataset\n",
    "    \n",
    "    end  = time.time()  \n",
    "    svm_time = end -start\n",
    "    #SVM END\n",
    "    \n",
    "    # Classifier - Algorithm - Logistic Regression\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time() \n",
    "    \n",
    "    LogReg = LogisticRegression()\n",
    "    LogReg.fit(Train_X_Tfidf,Train_Y)\n",
    "    predictions_LR = LogReg.predict(Test_X_Tfidf) # predict the labels on validation dataset\n",
    "    \n",
    "    end = time.time()\n",
    "    lr_time = end -start\n",
    "    #lOGISTIC REGRESSION END\n",
    "\n",
    "    #We compute the precison, recall and fscore of each of the models\n",
    "    #We use Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for\n",
    "    #label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "    prf_NB = precision_recall_fscore_support(predictions_NB, Test_Y,average='weighted')\n",
    "    prf_SVM = precision_recall_fscore_support(predictions_SVM, Test_Y,average='weighted')\n",
    "    prf_LR = precision_recall_fscore_support(predictions_LR, Test_Y,average='weighted')\n",
    "    print(\"~~~~For N labels = \", label , \", Max features = \", max_features  ,\"~~~~~\\n\")\n",
    "    print(\"===Naive Bayes===\\nPrecision, Recall, F1-Score: \",  prf_NB[0]*100,prf_NB[1]*100,prf_NB[2]*100 )\n",
    "    print(\"Naive Bayes Accuracy Score -> \",accuracy_score(Test_Y, predictions_NB)*100)\n",
    "    print(\"===SVM===\\nPrecision, Recall, F1-Score: \",  prf_SVM[0]*100,prf_SVM[1]*100,prf_SVM[2]*100)\n",
    "    print(\"SVM Accuracy Score -> \",accuracy_score(Test_Y, predictions_SVM)*100)\n",
    "    print(\"===Logistic Regression===\\nPrecision, Recall, F1-Score: \",  prf_LR[0]*100,prf_LR[1]*100,prf_LR[2]*100)\n",
    "    print(\"LR Accuracy Score -> \",accuracy_score(Test_Y,predictions_LR )*100)\n",
    "    print(\"Time spent on tokenizing for bag of words: \", tok_time, \"\\n\")\n",
    "    print(\"Time spent on vectorizing for NB/SVM/LR\", vect_time, \"\\n\")\n",
    "    print(\"Time spent on predicting for NB/SVM/LR models (respectively): \", \"\\n\",\n",
    "          nb_time, \"/\", svm_time ,\"/\", lr_time,\"\\n\")\n",
    "    \n",
    "##Split Data and Vectorize(Train and Split Method)\n",
    "def data(path,label,max_features):\n",
    "    # Add the Data using pandas\n",
    "    Corpus = pd.read_csv(path,encoding='latin-1')\n",
    "    Corpus['Polarity'] = Corpus['Polarity'].apply(str) #converts the float string into string/obj for processing\n",
    "    Corpus.dropna()\n",
    "    #print(Corpus.shape)\n",
    "    # Step - 1a : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    Corpus['TweetText']= [word_tokenize(str(entry)) for entry in Corpus['TweetText']]\n",
    "    # Step - 1b: Perfom Word Stemming/Lemmenting.\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(Corpus['TweetText']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "    #print(Corpus['text_final'].head())\n",
    "    # Step - 2: Split the model into Train and Test Data set\n",
    "    Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['Polarity'],test_size=0.2)\n",
    "\n",
    "    # Step - 3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "    # Step - 4: Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comaprison to the corpus\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=max_features)\n",
    "    Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "    return Train_X_Tfidf, Test_X_Tfidf, Train_Y,Test_Y\n",
    "# Use Stratify method to split data and vectorize (Used to compared with train split method)\n",
    "\n",
    "def stratifyData(path,label,max_features):\n",
    "    # Add the Data using pandas\n",
    "\n",
    "    Corpus = pd.read_csv(path,encoding='latin-1')\n",
    "    Corpus['Polarity'] = Corpus['Polarity'].apply(str) #converts the float string into string/obj for processing\n",
    "    Corpus.dropna()\n",
    "    #print(Corpus.shape)\n",
    "    # Step - 1a : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    Corpus['TweetText']= [word_tokenize(str(entry)) for entry in Corpus['TweetText']]\n",
    "    # Step - 1b: Perfom Word Stemming/Lemmenting.\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(Corpus['TweetText']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "    #print(Corpus['text_final'].head())\n",
    "    \n",
    "    # Step - 2: Split the model into Train and Test Data set\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    # X is the feature set and y is the target\n",
    "    for train_index, test_index in skf.split(X,y): \n",
    "        print(\"Train:\", train_index, \"Validation:\", val_index) \n",
    "        Train_X, Test_X = X[train_index], X[val_index] \n",
    "        Train_Y, Test_Y = y[train_index], y[val_index]\n",
    "    #Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['Polarity'],test_size=0.2)\n",
    "\n",
    "    # Step - 3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "    # Step - 4: Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comaprison to the corpus\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=max_features)\n",
    "    Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "    return Train_X_Tfidf, Test_X_Tfidf, Train_Y,Test_Y    \n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = ',   accuracy_score(test_labels, predictions)*100)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Used to Test model performance against ensembles using maxvoting method\n",
    "def maxVotingmodels(x_train, x_test, y_train, y_test ):\n",
    "    #Models Used\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    model1 = LogisticRegression(random_state=1)\n",
    "    model2 = DecisionTreeClassifier( random_state=1)\n",
    "    model3 = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "    model4 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    model5 = KNeighborsClassifier(n_neighbors=7)# KNN\n",
    "   \n",
    "    #knn\n",
    "    #boosted tree\n",
    "    #random forest\n",
    "    \n",
    "    #Voting classifiers, num indicates num of model inside\n",
    "    ens2 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),], voting='hard')\n",
    "    ens3 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive)], voting='hard')\n",
    "    ens4 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),], voting='hard')\n",
    "    ens5 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),('svm', model4)], voting='hard')\n",
    "    ens6 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),('svm', model4), ('knn', model5)], voting='hard')\n",
    " \n",
    "    Naive.fit(x_train,y_train)\n",
    "    model1.fit(x_train,y_train)\n",
    "    model2.fit(x_train,y_train)\n",
    "    model3.fit(x_train,y_train)\n",
    "    model4.fit(x_train,y_train)\n",
    "    model5.fit(x_train,y_train)\n",
    "    ens6.fit(x_train,y_train)\n",
    "    \n",
    "    print(\"6 model and all ensemble: Accuracy\")\n",
    "    for clf, label in zip([Naive, model1, model2,model3,model4,model5, ens2,ens3, ens4, ens5, ens6], ['Naive Bayes', 'Logistic Regression', 'Decision Tree Classifier', 'Random Forest Classifier', 'Support Vector Machine', 'K-Nearest Neighbour', 'Ensemble2', 'Ensemble3', 'Ensemble4', 'Ensemble5', 'Ensemble6']):\n",
    "        scores = cross_val_score(clf, x_test, y_test, scoring='accuracy', cv=5)\n",
    "        print(\"Accuracy: %0.6f (+/- %0.6f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "def maxVotingmodels2(x_train, x_test, y_train, y_test ):\n",
    "    #Models Used\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    model1 = LogisticRegression(random_state=1)\n",
    "    model2 = DecisionTreeClassifier( random_state=1)\n",
    "    model3 = RandomForestClassifier(n_estimators = 100, min_samples_split = 5, min_samples_leaf =3,\n",
    "                                   max_features= 'auto',max_depth=90, bootstrap= True)\n",
    "    model4 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    model5 = KNeighborsClassifier(n_neighbors=7)# KNN\n",
    "   \n",
    "    #knn\n",
    "    #boosted tree\n",
    "    #random forest\n",
    "    \n",
    "    #Voting classifiers, num indicates num of model inside\n",
    "    ens2 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),], voting='hard')\n",
    "    ens3 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive)], voting='hard')\n",
    "    ens4 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),], voting='hard')\n",
    "    ens5 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),('svm', model4)], voting='hard')\n",
    "    ens6 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),('svm', model4), ('knn', model5)], voting='hard')\n",
    " \n",
    "    Naive.fit(x_train,y_train)\n",
    "    model1.fit(x_train,y_train)\n",
    "    model2.fit(x_train,y_train)\n",
    "    model3.fit(x_train,y_train)\n",
    "    model4.fit(x_train,y_train)\n",
    "    model5.fit(x_train,y_train)\n",
    "    ens6.fit(x_train,y_train)\n",
    "    \n",
    "    print(\"6 model and all ensemble: Accuracy\")\n",
    "    for clf, label in zip([Naive, model1, model2,model3,model4,model5, ens2,ens3, ens4, ens5, ens6], ['Naive Bayes', 'Logistic Regression', 'Decision Tree Classifier', 'Random Forest Classifier', 'Support Vector Machine', 'K-Nearest Neighbour', 'Ensemble2', 'Ensemble3', 'Ensemble4', 'Ensemble5', 'Ensemble6']):\n",
    "        scores = cross_val_score(clf, x_test, y_test, scoring='accuracy', cv=5)\n",
    "        print(\"Accuracy: %0.6f (+/- %0.6f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "\n",
    "##Hyper parameter tuning\n",
    "\n",
    "\n",
    "\n",
    "##Qn 4.\n",
    "###################################\n",
    "\n",
    "##TESTS DONE: (Uncomment Sections to run)\n",
    "#Start Test 1\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750,5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet1500.csv',1500,5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet2250.csv',2250,5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet3000.csv',3000,5000)\n",
    "#END OF TEST\n",
    "\n",
    "#Start Test 3\n",
    "#calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet2250_30k.csv',2250,5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)\n",
    "#END OF TEST\n",
    "\n",
    "#Start Test 2\n",
    "##750 labelled data, change MF \n",
    "#calPerformanceofModels('./clean_healthcaretweet750.csv',750, 5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet750.csv',750, 2500)\n",
    "#calPerformanceofModels('./clean_healthcaretweet750.csv',750, 7500)\n",
    "#calPerformanceofModels('./clean_healthcaretweet750.csv',750, 10000)\n",
    "##1500 labelled data, change MF \n",
    "#calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 2500)\n",
    "#calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 7500)\n",
    "#calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 10000)\n",
    "##2250 labelled data, change MF \n",
    "#calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 2500)\n",
    "#calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 7500)\n",
    "#calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 10000)\n",
    "##3000 labelled data, change MF \n",
    "#calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 5000)\n",
    "#calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 2500)\n",
    "#calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 7500)\n",
    "#calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 10000)\n",
    "#END OF TEST\n",
    "\n",
    "#Start Test 4\n",
    "# #750 labelled data, change MF \n",
    "#calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,2500)\n",
    "# calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "# calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,7500)\n",
    "# calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,10000)\n",
    "# #1500 labelled data, change MF \n",
    "# calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,2500)\n",
    "# calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "# calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,7500)\n",
    "# calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,10000)\n",
    "# #2250 labelled data, change MF \n",
    "# ## Validate Results using clean_healthcaretweet1500 dataset\n",
    "# calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,2500)\n",
    "# calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)\n",
    "# calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,7500)\n",
    "# calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,10000)\n",
    "# #3000 labelled data, change MF \n",
    "\n",
    "#END OF TEST\n",
    "#Start Test 5\n",
    "# #750\n",
    "# print(\"Before-----------------\\n\")\n",
    "#calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "# print(\"After-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)\n",
    "# #1500\n",
    "# print(\"Before-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "# print(\"After-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet1500_30k.csv',1500,5000)\n",
    "# #3000\n",
    "# print(\"Before-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)\n",
    "# print(\"After-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet3000_30k.csv',3000,5000)\n",
    "#END OF TEST\n",
    "\n",
    "## Scaling Tests (uncomment to run)\n",
    "## Removing Stopwords\n",
    "#print(\"15k with stopwords-----------------\\n\")\n",
    "#calPerformanceofModels('./clean_healthcaretweet750.csv',750,5000)\n",
    "# print(\"15k without stopwords-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,5000)\n",
    "# print(\"30k with stopwords-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "# print(\"30k without stopwords-----------------\\n\")\n",
    "# print(\"30k-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)\n",
    "\n",
    "#Scaling, Increasing MF\n",
    "#print(\"15k without stopwords MF increased-----------------\\n\")\n",
    "#calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,5000)\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,7500)\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,10000)\n",
    "# print(\"30k without stopwords MF increased-----------------\\n\")\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,7500)\n",
    "# calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,10000)\n",
    "\n",
    "#Qn5\n",
    "###################################################\n",
    "#Split Train Test\n",
    "#x_train, x_test, y_train, y_test = data('clean_healthcaretweet3000_30k.csv', 750,5000)\n",
    "#maxVotingmodels(x_train, x_test, y_train, y_test)\n",
    "# #Stratified K-fold\n",
    "# x_train1, x_test1, y_train1, y_test1 = data('clean_healthcaretweet3000_30k.csv', 750,5000)\n",
    "# maxVotingmodels(x_train1, x_test1, y_train1, y_test1)\n",
    "\n",
    "#5bi\n",
    "##Hyper Parameter Tuning (uncomment to tune will take around an hour)\n",
    "#Start of primary tuning:Random Search CV\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "# # Use the random grid to search for best hyperparameters\n",
    "# # First create the base model to tune\n",
    "# rf = RandomForestClassifier()\n",
    "# # Random search of parameters, using 3 fold cross validation, \n",
    "# # search across 100 different combinations, and use all available cores\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, \n",
    "#                                random_state=42, n_jobs = -1)\n",
    "# # Fit the random search model\n",
    "# rf_random.fit(x_train1, y_train1)\n",
    "# # Best Parameters for Random Forest\n",
    "# print(rf_random.best_params_)\n",
    "\n",
    "######SET BEST PARAM FOR PRIMARY TUNING AND TEST IMPROVEMENT\n",
    "# rf = RandomForestClassifier(n_estimators = 2000, min_samples_split = 2, min_samples_leaf =2 ,\n",
    "#                                    max_features= 'auto',max_depth=90, bootstrap= True)\n",
    "# rf.fit(x_train1, y_train1)\n",
    "# base_accuracy = evaluate(rf, x_test1, y_test1)\n",
    "# # End of primary tuning\n",
    "\n",
    "\n",
    "#5bii\n",
    "\n",
    "## Secondary/Final Tuning: GridSearch CV\n",
    "\n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [80, 90, 100, 110],\n",
    "#     'max_features': ['auto'],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [3, 4, 5],\n",
    "#     'n_estimators': [100, 200, 300, 1000, 1500, 2250, 2500 ,3000]\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1,\n",
    "#                            verbose = 2)\n",
    "# grid_search.fit(x_train1 ,y_train1)\n",
    "# print(grid_search.best_params_)\n",
    "# rf = RandomForestClassifier(n_estimators = 100, min_samples_split = 5, min_samples_leaf =3,\n",
    "#                                    max_features= 'auto',max_depth=90, bootstrap= True)\n",
    "# rf.fit(x_train1, y_train1)\n",
    "# base_accuracy = evaluate(rf, x_test1, y_test1)\n",
    "# maxVotingmodels2(x_train1, x_test1, y_train1, y_test1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
